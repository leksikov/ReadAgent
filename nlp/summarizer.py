from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)

from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from pathlib import Path
import logging
import asyncio

logging.basicConfig(level=logging.DEBUG)



CHAT_MODEL = "gpt-4o-mini"
TEMPERATURE = 0.6
MAX_TOKENS = 1024

ENTITY = "Assistant AI"
TEMPLATE = """You are an {entity} tasked with shortening the long text.
Please shorten the following passage.
Just give me a shortened version. DO NOT explain your
reason.
"""

chat_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(TEMPLATE).format(entity=ENTITY),
        HumanMessagePromptTemplate.from_template("Passage: ```\n{passage}\n```."),
    ]
)

class Summarizer:
    """
    Class for shortening the text.
    """

    def __init__(self,  temperature: float=0.6, max_tokens: int=1024):
        self.max_tokens = max_tokens
        self.model = ChatOpenAI(model=CHAT_MODEL, temperature=temperature, max_tokens=self.max_tokens)
        self.prompt = chat_template

    async def summarize(self, passage:str) -> str:
        """
        Generate a short version of given passage.

        Returns:
            str: The response generated by the ChatOpenAI model.
        """
        # Generate a random word
        max_retries = 3
        for attempt in range(max_retries):

            try:
                chain = self.prompt | self.model | StrOutputParser()

                response = await chain.ainvoke({"passage": passage})
                logging.info(f"Summarizer response: {response}, {passage}")
                return response
            except Exception as e:
                logging.error(f"Error in Summary chain {e}")
                logging.error(f"Attempt {attempt + 1} failed in Summary chain {e}")
                breakpoint()
                if attempt < max_retries - 1:  # Check if not the last attempt
                    await asyncio.sleep(2**attempt)  # Exponential backoff
                else:
                    logging.error("Max retry attempts reached, giving up on Summary chain.")
        return ""

