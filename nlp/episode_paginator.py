
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)

from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from pathlib import Path
import logging
import asyncio
from typing import List
import yaml
logging.basicConfig(level=logging.ERROR)



CHAT_MODEL = "gpt-4o-mini"
TEMPERATURE = 0.6
MAX_TOKENS = 1024 # This is the maximum generation token limit for GPT-4o-mini.

CHUNK_SIZE = 900  # This keeps chunks well within the token limit.
OVERLAP = 100  # This ensures some context is preserved across chunks.

ENTITY = "Assistant AI"
TEMPLATE = """You are an {entity} tasked with finding a logical breakpoints in a given text.
You are given a passage that is taken from a larger text
(article, book, ...) and some numbered labels between the
paragraphs in the passage. The paragraphs may have overlapped text.
Numbered labels are in angle brackets. For example, if
the label number is 19, it shows as ⟨19⟩ in text.
Please choose a label where it is natural to break reading.
The label can be a scene transition, the end of a dialogue,
the end of an argument, a narrative transition, etc.

Please answer in Yaml format only. With the 'breakpoint'  and 'rationale' fields.
For example, if ⟨57⟩ is a good point to break, answer with YAML like this:
breakpoint: 57
rationale: "This is the point where the scene transitions from the office to the park."
"""

chat_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(TEMPLATE).format(entity=ENTITY),
        HumanMessagePromptTemplate.from_template("Passage: ```\n{passage}\n```. Please choose a labels where it is natural to break reading."),
    ]
)

class EpisodePaginator:
    """
    Class for finding a logical breakpoint in a given text.
    """

    def __init__(self):
        self.model = ChatOpenAI(model=CHAT_MODEL, temperature=TEMPERATURE, max_tokens=MAX_TOKENS)
        self.prompt = chat_template



    def parse_breakpoint_response(self, response: str) -> tuple[int, str]:
        """
        Parses the response from the model to extract the breakpoint and its rationale.

        Parameters:
        - response (str): The response string from the model.

        Returns:
        - tuple[int, str]: A tuple containing the breakpoint (as an integer) and the rationale (as a string).
        """
        # Extract the YAML content from the response
        # Assuming the response is always formatted with '```yaml\n' at the start and '\n```' at the end
        yaml_content = response.strip('`')  # Removes the backticks around the yaml block
        yaml_content = yaml_content.replace('yaml', '')  # Removes the 'yaml' tag if present
        # Parse the YAML content

        parsed_yaml = yaml.safe_load(yaml_content)

        breakpoint = parsed_yaml.get('breakpoint')
        rationale = parsed_yaml.get('rationale')

        return breakpoint, rationale

    

    def chunk_text(self, passage: str, max_words: int, overlap: int) -> List[str]:
        start = 0
        end = max_words

        chunks = []
        
        while start < len(passage.split()):
            chunk = passage.split()[start:end]
            chunks.append(" ".join(chunk))

            start += max_words - overlap
            end += max_words - overlap
        
        return chunks
    
    async def process_and_generate(self, passage: str, min_words: int, max_words: int, overlap: int) -> tuple[int, str]:
        """
        Generate an answer given a text passage, handling large texts by chunking.

        Args:
            passage (str): The passage to find a breakpoint in.

        Returns:
            str: The aggregated response generated by the ChatOpenAI model.
        """
        # Split the passage into chunks
        chunks = self.chunk_text(passage, max_words, overlap)
        
        # Initialize a container for responses from each chunk

        rationales = []
        breakpoints = []
        # Process each chunk
        for chunk in chunks:
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    # Adjust the prompt if necessary to fit the chunk processing
                    chain = self.prompt | self.model | StrOutputParser()
                    response = await chain.ainvoke({"passage": chunk})

                    breakpoint, rationale = self.parse_breakpoint_response(response)
                    breakpoints.append(breakpoint)
                    rationales.append(rationale)

                    break  # Break the retry loop on success
                except Exception as e:
                    logging.error(f"Error processing chunk: {e}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2**attempt)  # Exponential backoff
                    else:
                        logging.error("Max retry attempts reached for a chunk.")
                        rationales = []  # Consider how to handle failed chunks
                        breakpoints = []
                        break
        return breakpoints, rationales
