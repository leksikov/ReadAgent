
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)

from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from pathlib import Path
import logging
import asyncio
from typing import List
import yaml


logging.basicConfig(level=logging.ERROR)




CHAT_MODEL = "gpt-4o-mini"
TEMPERATURE = 0.6
MAX_TOKENS = 1024

ENTITY = "Assistant AI"
TEMPLATE = """You are an helpful {entity} tasked with question answering over given text.
Answer a question based on the text you have read.
If you don't know the answer, you can ask for more information or answer with "I don't know". Don't make up an answer.
Provide response in a YAML format with 'answer'.
For example, if the answer is '42', answer like this:
answer: 42


If you don't know the answer, answer like this:
answer: "I don't know the answer."
"""

chat_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(TEMPLATE).format(entity=ENTITY),
        HumanMessagePromptTemplate.from_template("Text Memory: ```\n{content}\n```. Please provide an answer over given text.\nQuestion: {question}"),
    ]
)



TEMPLATE2 = """You are an helpful {entity} tasked with question answering over given text.
You are given an answers made by other AI models who read and reviewed the pices of text.
You should select the best and informative response from the given answers.
Or combine the answers to provide a better response.
Ignore all those answers which includes "I don't know the answer." or "The text doesn't mention it.". Focus on positive answers.
"""

chat_template2 = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template(TEMPLATE2).format(entity=ENTITY),
        HumanMessagePromptTemplate.from_template("Text Memory: ```\n{content}\n```. Please provide an answer over given text.\nQuestion: {question}"),
    ]
)
class QAchain:
    """
    Select pages to answer given user question.
    """

    def __init__(self):
        self.model = ChatOpenAI(model=CHAT_MODEL, temperature=TEMPERATURE, max_tokens=MAX_TOKENS)
        self.prompt = chat_template
        self.prompt2 = chat_template2


    def chunk_text(self, passage: str, max_words: int, overlap: int) -> List[str]:
        start = 0
        end = max_words

        chunks = []
        
        while start < len(passage.split()):
            chunk = passage.split()[start:end]
            chunks.append(" ".join(chunk))

            start += max_words - overlap
            end += max_words - overlap
        
        return chunks
    
    def parse_breakpoint_response(self, response: str) -> tuple[int, str]:
        """
        Parses the response from the model to extract the breakpoint and its rationale.

        Parameters:
        - response (str): The response string from the model.

        Returns:
        - tuple[int, str]: A tuple containing the breakpoint (as an integer) and the rationale (as a string).
        """
        # Extract the YAML content from the response
        # Assuming the response is always formatted with '```yaml\n' at the start and '\n```' at the end
        yaml_content = response.strip('`')  # Removes the backticks around the yaml block
        yaml_content = yaml_content.replace('yaml', '')  # Removes the 'yaml' tag if present
        # Parse the YAML content

        parsed_yaml = yaml.safe_load(yaml_content)

        answer = parsed_yaml.get('answer')


        return answer
    
    async def generate(self, content:str, question: str) -> str:
        """
        Generate an answer given text pages.

        Returns:
            str: The response generated by the ChatOpenAI model.
        """
        max_words = 2000
        overlap = 128
        # Split the passage into chunks
        chunks = self.chunk_text(content, max_words, overlap)
        
        # Initialize a container for responses from each chunk


        answers = []
        chain = self.prompt | self.model | StrOutputParser()
        # Process each chunk
        for chunk in chunks:
            # Generate a random word
            max_retries = 3
            for attempt in range(max_retries):

                try:
                    
                    response = await chain.ainvoke({"content": chunk, "question": question})
                    answer = self.parse_breakpoint_response(response)
                    answers.append(answer)
                    
                    logging.info(f"QA chain response: {answer},  {chunk}, {question}")
                except Exception as e:
                    breakpoint()
                    logging.error(f"Error in QA chain {e}")
                    logging.error(f"Attempt {attempt + 1} failed in QA chain {e}")
                    if attempt < max_retries - 1:  # Check if not the last attempt
                        await asyncio.sleep(2**attempt)  # Exponential backoff
                    else:
                        logging.error("Max retry attempts reached, giving up on QA chain.")
         
        # filter answers if they contain "I don't know the answer." or "The text doesn't mention it."
        answers = [answer for answer in answers if answer not in ["I don't know.", "The text doesn't mention"]]


        
    
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You AI who follows instructions."),
            ("user", "{input}")
        ])
        llm = self.model
        chain = prompt | llm 
        filtered_response = await chain.ainvoke({"input": f"Filter out negative answers only. {answers}"})

        try:
            chain = self.prompt2 | self.model | StrOutputParser()
            response = await chain.ainvoke({"content": filtered_response, "question": question})

            return response
        except Exception as e:
            logging.error(f"Error in QA chain {e}")
            
            return "I don't know the answer."

